{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[2K\u001b[?25h[1mFetching:\u001b[22m\u001b[39m [========================================>]  100.0 %.0 %\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.1/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.1/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefining constant UPOSTAG\n",
      "WARNING: redefining constant UDEPREL\n",
      "WARNING: redefining constant UPOSTAG\n",
      "WARNING: redefining constant UDEPREL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{String,Any} with 13 entries:\n",
       "  \"back\"       => Any[Float32[1.70814 0.787587 … -0.636378 -1.76298; 4.32346 5.…\n",
       "  \"word_vocab\" => Dict{AbstractString,Int64}(\"null\"=>8668,\"Secure\"=>7932,\"Puppy…\n",
       "  \"char_vocab\" => Dict('\\x00\\x00\\x00\\x42'=>37,'\\x00\\x00\\x20\\x1d'=>116,'\\x00\\x00…\n",
       "  \"eowchar\"    => '\\x00\\x00\\x00\\x13'\n",
       "  \"sowchar\"    => '\\x00\\x00\\x00\\x12'\n",
       "  \"forw\"       => Any[Float32[1.47578 0.055072 … -0.2823 -1.58338; -1.95253 -0.…\n",
       "  \"eosword\"    => \"</s>\"\n",
       "  \"sosword\"    => \"<s>\"\n",
       "  \"soft\"       => Array{Float32,2}[[-0.303084 -1.18525 … -0.281071 -0.2053; -0.…\n",
       "  \"unkword\"    => \"<unk>\"\n",
       "  \"cembed\"     => Float32[0.593419 0.190279 … 0.11003 -0.000173569; -0.236287 0…\n",
       "  \"char\"       => Any[Float32[0.0708071 -0.456333 … 0.557484 0.094816; -1.61625…\n",
       "  \"unkchar\"    => '\\x00\\x00\\x00\\x11'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.update(\"Knet\")\n",
    "using JLD\n",
    "include(\"types.jl\")\n",
    "include(\"pre_processing.jl\")\n",
    "\n",
    "d = JLD.load(\"pretrained_model.jld2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3685-element Array{Any,1}:\n",
       " Sanal parçacıklar sa bunların hiçbirini yapamazlar .                                                                                                                                                                                                                   \n",
       " Ona her şeyimi verdim .                                                                                                                                                                                                                                                \n",
       " Karşısında , pantolonu dizlerine dek ıslak , önlük torbası ham eriklerle dolu İbrahim dikiliyordu .                                                                                                                                                                    \n",
       " Nereden biliyorsunuz .                                                                                                                                                                                                                                                 \n",
       " Aradığımı Buldum Sandım .                                                                                                                                                                                                                                              \n",
       " Kahveleri bende içelim .                                                                                                                                                                                                                                               \n",
       " Ne diyeceğimi bilemedim .                                                                                                                                                                                                                                              \n",
       " Kanamıyordu ...                                                                                                                                                                                                                                                        \n",
       " Süpürgen nerede .                                                                                                                                                                                                                                                      \n",
       " Bir taksi bulduk .                                                                                                                                                                                                                                                     \n",
       " Mebrure birden heyecanlanmıştı .                                                                                                                                                                                                                                       \n",
       " \" Burada ki üst düzey görüşmelerimizde turizm için ellerinden geleni yapacaklarını söylediler .                                                                                                                                                                        \n",
       " Gel bak , sana evi göstereyim .                                                                                                                                                                                                                                        \n",
       " ⋮                                                                                                                                                                                                                                                                      \n",
       " - Bu senin şalın mış .                                                                                                                                                                                                                                                 \n",
       " Bu kuruluşların ciro büyüklüklerine göre yaklaşık elli trilyon luk bir gelir sağlanacak .                                                                                                                                                                              \n",
       " Bartın gecesinden soğuk bir yel esmeye başlamıştı .                                                                                                                                                                                                                    \n",
       " Yerleştir yerleştirme , devşir devşirme sınavları falan filan yok ...                                                                                                                                                                                                  \n",
       " Rolü başkasına verdik dediler .                                                                                                                                                                                                                                        \n",
       " Derken , ev sahibi beyan değiştiriyor .                                                                                                                                                                                                                                \n",
       " MART'I BEKLİYORLARDI .                                                                                                                                                                                                                                                 \n",
       " Almanya'dan parlamento heyeti gelmiş .                                                                                                                                                                                                                                 \n",
       " Üç ocak lı demirci dükkanı deyip geçme , fayton imalatından zengin bile olmuşlar .                                                                                                                                                                                     \n",
       " Geçici bütçe tasarısı , Meclis'e sunuldu .                                                                                                                                                                                                                             \n",
       " Oysa , beyaz peynirin böyle bir efsanesi olsa .                                                                                                                                                                                                                        \n",
       " Erdoğan'ın milletvekili ve başbakan olmasını sağlayacak düzenlemede , Sezer'in karşı çıktığı , doğrudan başbakanlık getirecek yüzdokuz . madde yerine ara seçimi düzenleyen yetmişsekiz . madde üzerinde değişiklik yapacaklarını belirten Şahin , şu mesajı verdi : . "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = load_conllu(\"tr_imst-ud-train.conllu\",v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "makewmodel (generic function with 1 method)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############ MODEL CREATION ##############\n",
    "\n",
    "\n",
    "# character based model utilities\n",
    "cembed(m) = m[1]\n",
    "wchar(m) = m[2]; bchar(m) = m[3];\n",
    "wforw(m) = m[4]; bforw(m) = m[5];\n",
    "wback(m) = m[6]; bback(m) = m[7];\n",
    "wsoft(m) = m[8]; bsoft(m) = m[9];\n",
    "\n",
    "# To-load uniquely trained word-based language model\n",
    "makewmodel1(d)=[ d[\"cembed\"],\n",
    "                 d[\"char\"][1],\n",
    "                 d[\"char\"][2],\n",
    "                 d[\"forw\"][1],\n",
    "                 d[\"forw\"][2],\n",
    "                 d[\"back\"][1],\n",
    "                 d[\"back\"][2],\n",
    "                 d[\"soft\"][1],\n",
    "                 d[\"soft\"][2] ]\n",
    "\n",
    "# In order to load the word-model either as gpu or normal Array\n",
    "function makewmodel(d)\n",
    "    d1 = makewmodel1(d)\n",
    "    if gpu() >= 0\n",
    "        return map(KnetArray, d1)\n",
    "    else\n",
    "        return map(Array, d1)\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = create_vocab(d)\n",
    "wmodel = makewmodel(d);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maptoint (generic function with 1 method)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function maptoint(sentences, v::Vocab)\n",
    "    MAXWORD = 32\n",
    "    wdict = empty!(v.idict) # it is already empty ?\n",
    "    cdict = v.cdict\n",
    "    unkcid = cdict[v.unkchar]\n",
    "    words = Vector{Int}[]\n",
    "    sents = Vector{Int}[]\n",
    "\n",
    "    maxwordlen = 0; maxsentlen = 0;\n",
    "    for w in (v.sosword, v.eosword)\n",
    "        wid = get!(wdict, w, 1+length(wdict))\n",
    "        word = Array{Int}(undef,length(w)) #Array(Int, length(w))\n",
    "        wordi = 0 # to check 2 byte characters\n",
    "        for c in w\n",
    "            word[wordi+=1] = get(cdict, c, unkcid)\n",
    "        end\n",
    "        (wordi != length(w)) && error(\"Missing in single word process\")\n",
    "        (wordi > maxwordlen) && (maxwordlen = wordi)\n",
    "        push!(words, word)\n",
    "    end\n",
    "\n",
    "    for s in sentences\n",
    "        sent = Array{Int}(undef,length(s.word))  #Array(Int, length(s.word))\n",
    "        senti = 0\n",
    "        for w in s.word\n",
    "            ndict = length(wdict)\n",
    "            wid = get!(wdict, w, 1+ndict)\n",
    "            sent[senti+=1] = wid\n",
    "            if wid == 1+ndict\n",
    "                word = Array{Int}(undef,length(w)) #Array(Int, length(w))\n",
    "                wordi = 0\n",
    "                for c in w\n",
    "                    word[wordi+=1] = get(cdict, c, unkcid)\n",
    "                end\n",
    "                (wordi != length(w)) && error(\"Missing in single word process\")\n",
    "                if wordi > MAXWORD; wordi=MAXWORD; word = word[1:wordi]; end;\n",
    "                (wordi > maxwordlen) && (maxwordlen = wordi) \n",
    "                push!(words, word)\n",
    "            end\n",
    "        end\n",
    "        @assert(senti == length(s.word))\n",
    "        (senti > maxsentlen) && (maxsentlen = senti)\n",
    "        push!(sents, sent)\n",
    "    end\n",
    "    @assert(length(wdict) == length(words))\n",
    "    return words, sents, maxwordlen, maxsentlen\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array{Int64,1}[[1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1], [1, 1, 1]  …  [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], Array{Int64,1}[[3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 9], [14, 15, 16, 17, 18, 19, 15, 20, 21, 22, 23, 24, 25, 26, 9], [27, 28, 9], [29, 30, 31, 9], [32, 33, 34, 9], [35, 36, 37, 9], [38, 39], [40, 41, 9], [42, 43, 44, 9]  …  [13779, 13780, 1124, 73, 13781, 13782, 9633, 9], [13783, 13784, 15, 13785, 13786, 13787, 606, 607, 99, 39], [13788, 9628, 1915, 215, 9], [13789, 15, 854, 658, 4761, 13790, 9], [13791, 13792, 9], [5308, 13793, 6209, 146, 9], [8698, 5901, 160, 13794, 7669, 3910, 13795, 15, 13796, 13797, 1161, 941, 13798, 9], [13799, 5739, 13800, 15, 10256, 13801, 9], [1076, 15, 3222, 13802, 613, 73, 13803, 1595, 9], [2787, 2948, 79, 13804, 11635, 6047, 13805, 15, 663, 912  …  13198, 58, 3996, 866, 15, 1028, 13808, 1982, 282, 9]], 32, 57)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words,sents,maxwordlen,maxsentlen = maptoint(corpus,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenbatch (generic function with 2 methods)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function tokenbatch(words, maxlen, sos, eos, pad=eos)\n",
    "    B = length(words) # batchsize\n",
    "    T = maxlen + 2\n",
    "    data = [ Array{Int}(undef,B) for t in 1:T ]\n",
    "    mask = [ Array{Float32}(undef,B) for t in 1:T ]\n",
    "    @inbounds for t in 1:T\n",
    "        for b in 1:B\n",
    "            N = length(words[b]) # wordlen\n",
    "            n = t - T + N + 1 # cursor \n",
    "            if n < 0\n",
    "                mask[t][b] = 0\n",
    "                data[t][b] = pad\n",
    "            else\n",
    "                mask[t][b] = 1\n",
    "                if n == 0\n",
    "                    data[t][b] = sos\n",
    "                elseif n <= N\n",
    "                    data[t][b] = words[b][n]\n",
    "                elseif n == N+1\n",
    "                    data[t][b] = eos\n",
    "                else\n",
    "                    error()\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return data, mask\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "goldbatch (generic function with 2 methods)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function goldbatch(sentences, maxlen, wdict, unkwid, pad=unkwid)\n",
    "    B = length(sentences)\n",
    "    T = maxlen\n",
    "    data = [ Array{Int}(undef,B) for t in 1:T ]\n",
    "    mask = [ Array{Float32}(undef,B) for t in 1:T ]\n",
    "    for t in 1:T\n",
    "        for b in 1:B\n",
    "            N = length(sentences[b])\n",
    "            n = t - T + N\n",
    "            if n <= 0\n",
    "                mask[t][b] = 0\n",
    "                data[t][b] = pad\n",
    "            else\n",
    "                mask[t][b] = 1\n",
    "                data[t][b] = get(wdict, sentences[b].word[n], unkwid)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return data, mask\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fillcvecs! (generic function with 1 method)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############# MODEL FORWARDING FUNCTIONALITY ############\n",
    "\n",
    "# col-major lstm\n",
    "function _lstm(weight, bias, hidden, cell, input; mask=nothing)\n",
    "    gates = weight * vcat(input, hidden) .+ bias\n",
    "    H = size(hidden, 1)\n",
    "    forget = sigm.(gates[1:H, :])\n",
    "    ingate = sigm.(gates[1+H:2H, :])\n",
    "    outgate = sigm.(gates[1+2H:3H, :])\n",
    "    change = tanh.(gates[1+3H:4H, :])\n",
    "    (mask != nothing) && (mask = reshape(mask, 1, length(mask)))\n",
    "\n",
    "    cell = cell .* forget + ingate .* change\n",
    "    hidden = outgate .* tanh.(cell)\n",
    "    if mask != nothing\n",
    "        hidden = hidden .* mask\n",
    "        cell = cell .* mask\n",
    "    end\n",
    "    return (hidden, cell)\n",
    "end\n",
    "\n",
    "\n",
    "# Token-batched words are coming as input to the system, col-major lstm\n",
    "function charlstm(model, data, mask)\n",
    "    weight, bias, embeddings = wchar(model), bchar(model), cembed(model)\n",
    "    T = length(data)\n",
    "    B = length(data[1])\n",
    "    H = div(length(bias), 4)\n",
    "\n",
    "    \n",
    "    if isa(weight, KnetArray)\n",
    "        mask = map(KnetArray, mask)\n",
    "    end\n",
    "    \n",
    "    czero = fill!(similar(bias, H, B), 0)\n",
    "    hidden = cell = czero\n",
    "    for t in 1:T\n",
    "        (hidden, cell) = _lstm(weight, bias, hidden, cell, embeddings[:, data[t]]; mask=mask[t])\n",
    "    end\n",
    "    return hidden\n",
    "end\n",
    "\n",
    "\n",
    "# col-major bilstm implementation\n",
    "function wordlstm(model, data, mask, embeddings)\n",
    "    weight, bias = wforw(model), bforw(model)\n",
    "    T = length(data)\n",
    "    B = length(data[1])\n",
    "    H = div(length(bias), 4)\n",
    "\n",
    "\n",
    "    if isa(weight, KnetArray)\n",
    "        mask = map(KnetArray, mask)\n",
    "    end\n",
    "    \n",
    "    wzero = fill!(similar(bias, H, B), 0)\n",
    "\n",
    "    # forward lstm\n",
    "    hidden = cell = wzero\n",
    "    fhiddens = Array{Any}(T-2)  # fhiddens = Array(Any, T-2) : deprecated\n",
    "    for t in 1:T-2\n",
    "        (hidden, cell) = _lstm(weight, bias, hidden, cell, embeddings[:, data[t]]; mask=mask[t])\n",
    "        fhiddens[t] = hidden\n",
    "    end\n",
    "\n",
    "    # backward lstm\n",
    "    weight_b, bias_b = wback(model), bback(model)\n",
    "    hidden = cell = wzero\n",
    "    bhiddens = Array{Any}(T-2)  # bhiddens = Array(Any, T-2) : deprecated\n",
    "    for t in T:-1:3\n",
    "        (hidden, cell) = _lstm(weight_b, bias_b, hidden, cell, embeddings[:, data[t]]; mask=mask[t])\n",
    "        bhiddens[t-2] = hidden\n",
    "    end\n",
    "    return fhiddens, bhiddens\n",
    "end\n",
    "\n",
    "\n",
    "# col-major loss function implementation, as explained in paper\n",
    "function lmloss(model, data, mask, forw, back; result=nothing)\n",
    "    T = length(data)\n",
    "    B = length(data[1])\n",
    "    weight, bias = wsoft(model), bsoft(model)\n",
    "    idx(t,b,n) = data[t][b] + (b-1)*n\n",
    "\n",
    "    total = count = 0\n",
    "    for t in 1:T\n",
    "        ypred = weight * vcat(forw[t], back[t]) .+ bias\n",
    "        nrows,ncols = size(ypred)\n",
    "        index = Int[]\n",
    "        for b=1:B\n",
    "            if mask[t][b]==1\n",
    "                push!(index, idx(t,b,nrows))\n",
    "            end\n",
    "        end\n",
    "        o1 = logp(ypred, dims=1)\n",
    "        o2 = o1[index]\n",
    "        total += sum(o2)\n",
    "        count += length(o2)\n",
    "    end\n",
    "    \n",
    "    if result != nothing\n",
    "        result[1] += AutoGrad.getval(total)\n",
    "        result[2] += count\n",
    "    end\n",
    "    return total\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "################# To COMPUTE RELATED EMBEDDINGS ###################\n",
    "\n",
    "\n",
    "# To fill the constant word embeddings\n",
    "function fillwvecs!(sentences, isents, wembed; GPUFEATS=false)\n",
    "    for (s, isents) in zip(sentences, isents)\n",
    "        empty!(s.wvec)\n",
    "        for w in isents\n",
    "            if GPUFEATS\n",
    "                push!(s.wvec, wembed[:, w])\n",
    "            else\n",
    "                push!(s.wvec, Array(wembed[:, w]))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "# To fill the context embeddings of words in given sentences one by one\n",
    "function fillcvecs!(sentences, forw, back; GPUFEATS=false)\n",
    "    T = length(forw)\n",
    "    for i in 1:length(sentences)\n",
    "        s = sentences[i]\n",
    "        empty!(s.fvec)\n",
    "        empty!(s.bvec)\n",
    "        N = length(s)\n",
    "        for n in 1:N\n",
    "            t = T-N+n\n",
    "            if GPUFEATS #GPU\n",
    "                push!(s.fvec, forw[t][:,i])\n",
    "                push!(s.bvec, back[t][:,i])\n",
    "            else #CPU\n",
    "                push!(s.fvec, Array(forw[t][:,i]))\n",
    "                push!(s.bvec, Array(back[t][:,i]))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fillvecs! (generic function with 1 method)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function fillvecs!(wmodel, sentences, vocab; batchsize=128)\n",
    "\n",
    "    words, sents, maxwordlen, maxsentlen = maptoint(sentences, vocab)\n",
    "    sow = vocab.cdict[vocab.sowchar]\n",
    "    eow = vocab.cdict[vocab.eowchar]\n",
    "    paw = vocab.cdict[vocab.unkchar]\n",
    " \n",
    "    # word-embeddings calcutation\n",
    "    wembed = Any[]\n",
    "    #free_KnetArray();\n",
    "    for i=1:batchsize:length(words)\n",
    "        j = min(i+batchsize-1,length(words))\n",
    "        wij = view(words,i:j)\n",
    "        maxij = maximum(map(length, wij))\n",
    "        cdata, cmask = tokenbatch(wij, maxij, sow, eow)\n",
    "        push!(wembed, charlstm(wmodel, cdata, cmask))\n",
    "    end\n",
    "    wembed =hcat(wembed...) # Here I applied the changes from hcatn-> hcat in newer version\n",
    "    fillwvecs!(sentences, sents, wembed)\n",
    "\n",
    "    sos,eos,unk = vocab.idict[vocab.sosword], vocab.idict[vocab.eosword], vocab.odict[vocab.unkword]\n",
    "    result = zeros(2)\n",
    "    #free_KnetArray()\n",
    "    for i=1:batchsize:length(sents)\n",
    "        j = min(i+batchsize-1, length(sents))\n",
    "        isentij = view(sents, i:j)\n",
    "        maxij = maximum(map(length, isentij))\n",
    "        wdata, wmask = tokenbatch(isentij, maxij, sos, eos)\n",
    "        forw, back = wordlstm(wmodel, wdata, wmask, wembed)\n",
    "        sentij = view(sentences, i:j)\n",
    "        fillcvecs!(sentij, forw, back)\n",
    "        odata, omask = goldbatch(sentij, maxij, vocab.odict, unk)\n",
    "        lmloss(wmodel,odata,omask,forw,back; result=result) \n",
    "    end\n",
    "    return exp(-result[1]/result[2])\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching Array{Any,N} where N(::Int64)\nClosest candidates are:\n  Array{Any,N} where N(!Matched::UndefInitializer, !Matched::Int64) where T at boot.jl:416\n  Array{Any,N} where N(!Matched::UndefInitializer, !Matched::Int64, !Matched::Int64) where T at boot.jl:417\n  Array{Any,N} where N(!Matched::UndefInitializer, !Matched::Int64, !Matched::Int64, !Matched::Int64) where T at boot.jl:418\n  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching Array{Any,N} where N(::Int64)\nClosest candidates are:\n  Array{Any,N} where N(!Matched::UndefInitializer, !Matched::Int64) where T at boot.jl:416\n  Array{Any,N} where N(!Matched::UndefInitializer, !Matched::Int64, !Matched::Int64) where T at boot.jl:417\n  Array{Any,N} where N(!Matched::UndefInitializer, !Matched::Int64, !Matched::Int64, !Matched::Int64) where T at boot.jl:418\n  ...",
      "",
      "Stacktrace:",
      " [1] wordlstm(::Array{Array{Float32,2},1}, ::Array{Array{Int64,1},1}, ::Array{Array{Float32,1},1}, ::Array{Float32,2}) at ./In[110]:60",
      " [2] #fillvecs!#99(::Int64, ::Function, ::Array{Array{Float32,2},1}, ::Array{Any,1}, ::Vocab) at ./In[111]:29",
      " [3] fillvecs!(::Array{Array{Float32,2},1}, ::Array{Any,1}, ::Vocab) at ./In[111]:3",
      " [4] top-level scope at In[112]:1"
     ]
    }
   ],
   "source": [
    "fillvecs!(wmodel,corpus, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
